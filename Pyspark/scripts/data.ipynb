{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f4c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/Applications/spark\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f39ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 23:04:14 WARN Utils: Your hostname, Kavis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.24 instead (on interface en0)\n",
      "26/01/24 23:04:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/kavisanthoshkumar/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kavisanthoshkumar/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4a5e1471-fd02-436e-9b73-4d731fc6c9b3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Applications/Spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 111ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4a5e1471-fd02-436e-9b73-4d731fc6c9b3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "26/01/24 23:04:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark-ml\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3031def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    IntegerType, \n",
    "    FloatType, \n",
    "    StringType, \n",
    "    DoubleType, \n",
    "    BooleanType\n",
    ")\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7372b9",
   "metadata": {},
   "source": [
    "#### 1.\tLoad the CSV file with header and infer schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da7c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Administrative: integer (nullable = true)\n",
      " |-- Administrative_Duration: double (nullable = true)\n",
      " |-- Informational: integer (nullable = true)\n",
      " |-- Informational_Duration: double (nullable = true)\n",
      " |-- ProductRelated: integer (nullable = true)\n",
      " |-- ProductRelated_Duration: double (nullable = true)\n",
      " |-- BounceRates: double (nullable = true)\n",
      " |-- ExitRates: double (nullable = true)\n",
      " |-- PageValues: double (nullable = true)\n",
      " |-- SpecialDay: double (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- OperatingSystems: integer (nullable = true)\n",
      " |-- Browser: integer (nullable = true)\n",
      " |-- Region: integer (nullable = true)\n",
      " |-- TrafficType: integer (nullable = true)\n",
      " |-- VisitorType: string (nullable = true)\n",
      " |-- Weekend: boolean (nullable = true)\n",
      " |-- Revenue: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"../dataset/online_shoppers_intention.csv\", header = True, inferSchema = True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a758fd9",
   "metadata": {},
   "source": [
    "#### 2.\tLoad the CSV again using an explicitly defined schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdace44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Administrative: integer (nullable = true)\n",
      " |-- Administrative_Duration: double (nullable = true)\n",
      " |-- Informational: integer (nullable = true)\n",
      " |-- Informational_Duration: double (nullable = true)\n",
      " |-- ProductRelated: integer (nullable = true)\n",
      " |-- ProductRelated_Duration: double (nullable = true)\n",
      " |-- BounceRates: double (nullable = true)\n",
      " |-- ExitRates: double (nullable = true)\n",
      " |-- PageValues: double (nullable = true)\n",
      " |-- SpecialDay: double (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- OperatingSystems: integer (nullable = true)\n",
      " |-- Browser: integer (nullable = true)\n",
      " |-- Region: integer (nullable = true)\n",
      " |-- TrafficType: integer (nullable = true)\n",
      " |-- VisitorType: string (nullable = true)\n",
      " |-- Weekend: boolean (nullable = true)\n",
      " |-- Revenue: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_schema = StructType([\n",
    "    StructField(\"Administrative\", IntegerType()), \n",
    "    StructField(\"Administrative_Duration\", DoubleType()), \n",
    "    StructField(\"Informational\", IntegerType()), \n",
    "    StructField(\"Informational_Duration\", DoubleType()), \n",
    "    StructField(\"ProductRelated\", IntegerType()), \n",
    "    StructField(\"ProductRelated_Duration\", DoubleType()), \n",
    "    StructField(\"BounceRates\", DoubleType()), \n",
    "    StructField(\"ExitRates\", DoubleType()), \n",
    "    StructField(\"PageValues\", DoubleType()), \n",
    "    StructField(\"SpecialDay\", DoubleType()),\n",
    "    StructField(\"Month\", StringType()), \n",
    "    StructField(\"OperatingSystems\", IntegerType()), \n",
    "    StructField(\"Browser\", IntegerType()),\n",
    "    StructField(\"Region\", IntegerType()), \n",
    "    StructField(\"TrafficType\",IntegerType()), \n",
    "    StructField(\"VisitorType\", StringType()), \n",
    "    StructField(\"Weekend\", BooleanType()),\n",
    "    StructField(\"Revenue\", BooleanType())\n",
    "])\n",
    "\n",
    "\n",
    "data = spark.read.csv(\"../dataset/online_shoppers_intention.csv\", schema = data_schema, header = True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd47f8b",
   "metadata": {},
   "source": [
    "#### 3.\tAdd a column ingestion_timestamp using current timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5d675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Date : 2026-01-24\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().date()\n",
    "print(f\"Ingestion Date : {current_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3471d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|ingestion_timestamp|\n",
      "+-------------------+\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "|         2026-01-24|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"ingestion_timestamp\", F.lit(current_date))\n",
    "\n",
    "data.select('ingestion_timestamp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03f302",
   "metadata": {},
   "source": [
    "#### 4.\tRename column Revenue to made_purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40d9514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Administrative: integer (nullable = true)\n",
      " |-- Administrative_Duration: double (nullable = true)\n",
      " |-- Informational: integer (nullable = true)\n",
      " |-- Informational_Duration: double (nullable = true)\n",
      " |-- ProductRelated: integer (nullable = true)\n",
      " |-- ProductRelated_Duration: double (nullable = true)\n",
      " |-- BounceRates: double (nullable = true)\n",
      " |-- ExitRates: double (nullable = true)\n",
      " |-- PageValues: double (nullable = true)\n",
      " |-- SpecialDay: double (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- OperatingSystems: integer (nullable = true)\n",
      " |-- Browser: integer (nullable = true)\n",
      " |-- Region: integer (nullable = true)\n",
      " |-- TrafficType: integer (nullable = true)\n",
      " |-- VisitorType: string (nullable = true)\n",
      " |-- Weekend: boolean (nullable = true)\n",
      " |-- made_purchase: boolean (nullable = true)\n",
      " |-- ingestion_timestamp: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumnRenamed(\"Revenue\", \"made_purchase\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681755e",
   "metadata": {},
   "source": [
    "#### 5. Convert Month to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b22bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Month|\n",
      "+-----+\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "|  feb|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"Month\", F.lower(F.col(\"Month\")))\n",
    "\n",
    "data.select(\"Month\").show() # all the entries are converted to lowercase characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f3456",
   "metadata": {},
   "source": [
    "#### 6.\tDrop records where VisitorType is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac4b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      VisitorType|\n",
      "+-----------------+\n",
      "|      New_Visitor|\n",
      "|            Other|\n",
      "|Returning_Visitor|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"VisitorType\").distinct().show()\n",
    "# We can see only three different categories in VisitorType column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780336ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+-------------+----------------------+--------------+-----------------------+-----------+---------+----------+----------+-----+----------------+-------+------+-----------+-----------+-------+-------------+-------------------+\n",
      "|Administrative|Administrative_Duration|Informational|Informational_Duration|ProductRelated|ProductRelated_Duration|BounceRates|ExitRates|PageValues|SpecialDay|Month|OperatingSystems|Browser|Region|TrafficType|VisitorType|Weekend|made_purchase|ingestion_timestamp|\n",
      "+--------------+-----------------------+-------------+----------------------+--------------+-----------------------+-----------+---------+----------+----------+-----+----------------+-------+------+-----------+-----------+-------+-------------+-------------------+\n",
      "+--------------+-----------------------+-------------+----------------------+--------------+-----------------------+-----------+---------+----------+----------+-----+----------------+-------+------+-----------+-----------+-------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping th records where VisitorType is null\n",
    "data = data[F.col(\"VisitorType\").isNull()]\n",
    "# Currently we don't have any records with null entires     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b9c7e",
   "metadata": {},
   "source": [
    "#### 7.\tCount total number of sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0063cd5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sessions` cannot be resolved. Did you mean one of the following? [`Region`, `Month`, `Weekend`, `Browser`, `ExitRates`].;\n'Project ['sessions]\n+- Project [Administrative#53, Administrative_Duration#54, Informational#55, Informational_Duration#56, ProductRelated#57, ProductRelated_Duration#58, BounceRates#59, ExitRates#60, PageValues#61, SpecialDay#62, Month#63, OperatingSystems#64, Browser#65, Region#66, TrafficType#67, VisitorType#68, Weekend#69, Revenue#70 AS made_purchase#238, ingestion_timestamp#89]\n   +- Project [Administrative#53, Administrative_Duration#54, Informational#55, Informational_Duration#56, ProductRelated#57, ProductRelated_Duration#58, BounceRates#59, ExitRates#60, PageValues#61, SpecialDay#62, Month#63, OperatingSystems#64, Browser#65, Region#66, TrafficType#67, VisitorType#68, Weekend#69, Revenue#70, 2026-01-24 AS ingestion_timestamp#89]\n      +- Relation [Administrative#53,Administrative_Duration#54,Informational#55,Informational_Duration#56,ProductRelated#57,ProductRelated_Duration#58,BounceRates#59,ExitRates#60,PageValues#61,SpecialDay#62,Month#63,OperatingSystems#64,Browser#65,Region#66,TrafficType#67,VisitorType#68,Weekend#69,Revenue#70] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pyspark/sql/dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `sessions` cannot be resolved. Did you mean one of the following? [`Region`, `Month`, `Weekend`, `Browser`, `ExitRates`].;\n'Project ['sessions]\n+- Project [Administrative#53, Administrative_Duration#54, Informational#55, Informational_Duration#56, ProductRelated#57, ProductRelated_Duration#58, BounceRates#59, ExitRates#60, PageValues#61, SpecialDay#62, Month#63, OperatingSystems#64, Browser#65, Region#66, TrafficType#67, VisitorType#68, Weekend#69, Revenue#70 AS made_purchase#238, ingestion_timestamp#89]\n   +- Project [Administrative#53, Administrative_Duration#54, Informational#55, Informational_Duration#56, ProductRelated#57, ProductRelated_Duration#58, BounceRates#59, ExitRates#60, PageValues#61, SpecialDay#62, Month#63, OperatingSystems#64, Browser#65, Region#66, TrafficType#67, VisitorType#68, Weekend#69, Revenue#70, 2026-01-24 AS ingestion_timestamp#89]\n      +- Relation [Administrative#53,Administrative_Duration#54,Informational#55,Informational_Duration#56,ProductRelated#57,ProductRelated_Duration#58,BounceRates#59,ExitRates#60,PageValues#61,SpecialDay#62,Month#63,OperatingSystems#64,Browser#65,Region#66,TrafficType#67,VisitorType#68,Weekend#69,Revenue#70] csv\n"
     ]
    }
   ],
   "source": [
    "data.select(\"sessions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578e36a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Administrative: integer (nullable = true)\n",
      " |-- Administrative_Duration: double (nullable = true)\n",
      " |-- Informational: integer (nullable = true)\n",
      " |-- Informational_Duration: double (nullable = true)\n",
      " |-- ProductRelated: integer (nullable = true)\n",
      " |-- ProductRelated_Duration: double (nullable = true)\n",
      " |-- BounceRates: double (nullable = true)\n",
      " |-- ExitRates: double (nullable = true)\n",
      " |-- PageValues: double (nullable = true)\n",
      " |-- SpecialDay: double (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- OperatingSystems: integer (nullable = true)\n",
      " |-- Browser: integer (nullable = true)\n",
      " |-- Region: integer (nullable = true)\n",
      " |-- TrafficType: integer (nullable = true)\n",
      " |-- VisitorType: string (nullable = true)\n",
      " |-- Weekend: boolean (nullable = true)\n",
      " |-- made_purchase: boolean (nullable = true)\n",
      " |-- ingestion_timestamp: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59075e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
